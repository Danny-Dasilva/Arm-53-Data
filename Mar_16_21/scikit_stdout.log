Architecture:        aarch64
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3
Thread(s) per core:  1
Core(s) per socket:  4
Socket(s):           1
NUMA node(s):        1
Vendor ID:           ARM
Model:               4
Model name:          Cortex-A53
Stepping:            r0p4
CPU max MHz:         1500.0000
CPU min MHz:         500.0000
BogoMIPS:            16.66
L1d cache:           unknown size
L1i cache:           unknown size
L2 cache:            unknown size
NUMA node0 CPU(s):   0-3
Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid
MemTotal:        1009596 kB
================================================================================
================================================================================
Running run_forest_importances_faces test
perf stat -o ../output/run_forest_importances_faces.log --per-core -a taskset -c 0-3 ./run_forest_importances_faces.sh -n 4

=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.

=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.

=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.



=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.


/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
done in 7.194s
done in 7.207s
done in 7.225s
done in 7.203s
./run_forest_importances_faces.sh: line 32:   508 Killed                  python3 plot_forest_importances_faces.py
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 23:12:47 2021


 Performance counter stats for 'system wide':

S0-C0           1           18925.52 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              16837      context-switches          #    0.890 K/sec                  
S0-C0           1                801      cpu-migrations            #    0.042 K/sec                  
S0-C0           1              32883      page-faults               #    0.002 M/sec                  
S0-C0           1        21133039810      cycles                    #    1.117 GHz                    
S0-C0           1         7224127786      instructions              #    0.34  insn per cycle         
S0-C0           1          856743869      branches                  #   45.269 M/sec                  
S0-C0           1          150322043      branch-misses             #   17.55% of all branches        
S0-C1           1           18925.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              14046      context-switches          #    0.742 K/sec                  
S0-C1           1                892      cpu-migrations            #    0.047 K/sec                  
S0-C1           1              38901      page-faults               #    0.002 M/sec                  
S0-C1           1        22913802461      cycles                    #    1.211 GHz                    
S0-C1           1         8250650341      instructions              #    0.36  insn per cycle         
S0-C1           1          959855554      branches                  #   50.718 M/sec                  
S0-C1           1          140515504      branch-misses             #   14.64% of all branches        
S0-C2           1           18925.50 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              15819      context-switches          #    0.836 K/sec                  
S0-C2           1                941      cpu-migrations            #    0.050 K/sec                  
S0-C2           1              48787      page-faults               #    0.003 M/sec                  
S0-C2           1        21824943449      cycles                    #    1.153 GHz                    
S0-C2           1         7848688402      instructions              #    0.36  insn per cycle         
S0-C2           1          900911871      branches                  #   47.603 M/sec                  
S0-C2           1          142084348      branch-misses             #   15.77% of all branches        
S0-C3           1           18925.55 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              14093      context-switches          #    0.745 K/sec                  
S0-C3           1                806      cpu-migrations            #    0.043 K/sec                  
S0-C3           1              52864      page-faults               #    0.003 M/sec                  
S0-C3           1        22043500814      cycles                    #    1.165 GHz                    
S0-C3           1         7705376706      instructions              #    0.35  insn per cycle         
S0-C3           1          874492058      branches                  #   46.207 M/sec                  
S0-C3           1          140118824      branch-misses             #   16.02% of all branches        

      18.928141416 seconds time elapsed

================================================================================
Running run_multioutput_face_completion test
perf stat -o ../output/run_multioutput_face_completion.log --per-core -a taskset -c 0-3 ./run_multioutput_face_completion.sh -n 4

==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.





==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
./run_multioutput_face_completion.sh: line 35:   517 Killed                  python3 plot_multioutput_face_completion.py
./run_multioutput_face_completion.sh: line 35:   520 Killed                  python3 plot_multioutput_face_completion.py
./run_multioutput_face_completion.sh: line 35:   519 Killed                  python3 plot_multioutput_face_completion.py
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 23:13:06 2021


 Performance counter stats for 'system wide':

S0-C0           1           43208.08 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              23902      context-switches          #    0.553 K/sec                  
S0-C0           1                937      cpu-migrations            #    0.022 K/sec                  
S0-C0           1              38705      page-faults               #    0.896 K/sec                  
S0-C0           1        36331823476      cycles                    #    0.841 GHz                    
S0-C0           1        15752746667      instructions              #    0.43  insn per cycle         
S0-C0           1         1208373909      branches                  #   27.966 M/sec                  
S0-C0           1          135602369      branch-misses             #   11.22% of all branches        
S0-C1           1           43208.08 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              25919      context-switches          #    0.600 K/sec                  
S0-C1           1               1209      cpu-migrations            #    0.028 K/sec                  
S0-C1           1              74437      page-faults               #    0.002 M/sec                  
S0-C1           1        55243114386      cycles                    #    1.279 GHz                    
S0-C1           1        26853403751      instructions              #    0.49  insn per cycle         
S0-C1           1         1759682969      branches                  #   40.726 M/sec                  
S0-C1           1          117455624      branch-misses             #    6.67% of all branches        
S0-C2           1           43208.08 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              20984      context-switches          #    0.486 K/sec                  
S0-C2           1               1176      cpu-migrations            #    0.027 K/sec                  
S0-C2           1              62890      page-faults               #    0.001 M/sec                  
S0-C2           1        32743267785      cycles                    #    0.758 GHz                    
S0-C2           1        12446848201      instructions              #    0.38  insn per cycle         
S0-C2           1         1359096592      branches                  #   31.455 M/sec                  
S0-C2           1           92361424      branch-misses             #    6.80% of all branches        
S0-C3           1           43208.07 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              20747      context-switches          #    0.480 K/sec                  
S0-C3           1               1009      cpu-migrations            #    0.023 K/sec                  
S0-C3           1              59208      page-faults               #    0.001 M/sec                  
S0-C3           1        42165012167      cycles                    #    0.976 GHz                    
S0-C3           1        18026233587      instructions              #    0.43  insn per cycle         
S0-C3           1         1648153978      branches                  #   38.145 M/sec                  
S0-C3           1          126179177      branch-misses             #    7.66% of all branches        

      43.210033923 seconds time elapsed

================================================================================
Running run_logistic_path test
perf stat -o ../output/run_logistic_path.log --per-core -a taskset -c 0-3 ./run_logistic_path.sh -n 4

==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.





==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Computing regularization path ...
Computing regularization path ...
Computing regularization path ...
Computing regularization path ...
This took 22.950s
This took 23.555s
This took 23.913s
This took 23.958s
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 23:13:49 2021


 Performance counter stats for 'system wide':

S0-C0           1           30640.57 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              13653      context-switches          #    0.446 K/sec                  
S0-C0           1                790      cpu-migrations            #    0.026 K/sec                  
S0-C0           1              22799      page-faults               #    0.744 K/sec                  
S0-C0           1        41302323149      cycles                    #    1.348 GHz                    
S0-C0           1        25866687851      instructions              #    0.63  insn per cycle         
S0-C0           1         2700813319      branches                  #   88.145 M/sec                  
S0-C0           1          163011196      branch-misses             #    6.04% of all branches        
S0-C1           1           30640.57 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              14058      context-switches          #    0.459 K/sec                  
S0-C1           1               1133      cpu-migrations            #    0.037 K/sec                  
S0-C1           1              30548      page-faults               #    0.997 K/sec                  
S0-C1           1        43698185807      cycles                    #    1.426 GHz                    
S0-C1           1        26303498703      instructions              #    0.60  insn per cycle         
S0-C1           1         2723132864      branches                  #   88.873 M/sec                  
S0-C1           1          146104330      branch-misses             #    5.37% of all branches        
S0-C2           1           30640.61 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              12718      context-switches          #    0.415 K/sec                  
S0-C2           1                884      cpu-migrations            #    0.029 K/sec                  
S0-C2           1              31635      page-faults               #    0.001 M/sec                  
S0-C2           1        43641263611      cycles                    #    1.424 GHz                    
S0-C2           1        26262462267      instructions              #    0.60  insn per cycle         
S0-C2           1         2719402901      branches                  #   88.752 M/sec                  
S0-C2           1          144959779      branch-misses             #    5.33% of all branches        
S0-C3           1           30640.66 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              11189      context-switches          #    0.365 K/sec                  
S0-C3           1                899      cpu-migrations            #    0.029 K/sec                  
S0-C3           1              31257      page-faults               #    0.001 M/sec                  
S0-C3           1        43045474493      cycles                    #    1.405 GHz                    
S0-C3           1        26269645600      instructions              #    0.61  insn per cycle         
S0-C3           1         2712017873      branches                  #   88.510 M/sec                  
S0-C3           1          145506758      branch-misses             #    5.37% of all branches        

      30.642152723 seconds time elapsed

================================================================================
Running run_plot_svm_nonlinear test
perf stat -o ../output/run_plot_svm_nonlinear.log --per-core -a taskset -c 0-3 ./run_plot_svm_nonlinear.sh -n 4

==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.


==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.


==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.


==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.

/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 23:14:20 2021


 Performance counter stats for 'system wide':

S0-C0           1           12570.28 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              11176      context-switches          #    0.889 K/sec                  
S0-C0           1                663      cpu-migrations            #    0.053 K/sec                  
S0-C0           1              24784      page-faults               #    0.002 M/sec                  
S0-C0           1        15459496066      cycles                    #    1.230 GHz                    
S0-C0           1         8893808229      instructions              #    0.58  insn per cycle         
S0-C0           1          969879255      branches                  #   77.157 M/sec                  
S0-C0           1           84497004      branch-misses             #    8.71% of all branches        
S0-C1           1           12570.29 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              11875      context-switches          #    0.945 K/sec                  
S0-C1           1               1041      cpu-migrations            #    0.083 K/sec                  
S0-C1           1              33943      page-faults               #    0.003 M/sec                  
S0-C1           1        16247591574      cycles                    #    1.293 GHz                    
S0-C1           1         9295087950      instructions              #    0.57  insn per cycle         
S0-C1           1          985960742      branches                  #   78.436 M/sec                  
S0-C1           1           69523171      branch-misses             #    7.05% of all branches        
S0-C2           1           12570.30 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              11926      context-switches          #    0.949 K/sec                  
S0-C2           1                920      cpu-migrations            #    0.073 K/sec                  
S0-C2           1              36925      page-faults               #    0.003 M/sec                  
S0-C2           1        16133317708      cycles                    #    1.283 GHz                    
S0-C2           1         9251934646      instructions              #    0.57  insn per cycle         
S0-C2           1          979641220      branches                  #   77.933 M/sec                  
S0-C2           1           68120182      branch-misses             #    6.95% of all branches        
S0-C3           1           12570.31 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              11410      context-switches          #    0.908 K/sec                  
S0-C3           1                906      cpu-migrations            #    0.072 K/sec                  
S0-C3           1              33288      page-faults               #    0.003 M/sec                  
S0-C3           1        16338330788      cycles                    #    1.300 GHz                    
S0-C3           1         9253652467      instructions              #    0.57  insn per cycle         
S0-C3           1          983248808      branches                  #   78.220 M/sec                  
S0-C3           1           66967954      branch-misses             #    6.81% of all branches        

      12.571843931 seconds time elapsed

================================================================================
Running run_plot_theilsen test
perf stat -o ../output/run_plot_theilsen.log --per-core -a taskset -c 0-3 ./run_plot_theilsen.sh -n 4
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp

====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.

./run_plot_theilsen.sh: line 32:   553 Killed                  python3 plot_theilsen.py
./run_plot_theilsen.sh: line 32:   556 Killed                  python3 plot_theilsen.py
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 23:14:33 2021


 Performance counter stats for 'system wide':

S0-C0           1           23018.09 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              18457      context-switches          #    0.802 K/sec                  
S0-C0           1                792      cpu-migrations            #    0.034 K/sec                  
S0-C0           1              40082      page-faults               #    0.002 M/sec                  
S0-C0           1        17049242301      cycles                    #    0.741 GHz                    
S0-C0           1         6334483509      instructions              #    0.37  insn per cycle         
S0-C0           1          848693939      branches                  #   36.871 M/sec                  
S0-C0           1          154004413      branch-misses             #   18.15% of all branches        
S0-C1           1           23018.09 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              17225      context-switches          #    0.748 K/sec                  
S0-C1           1               1104      cpu-migrations            #    0.048 K/sec                  
S0-C1           1              60820      page-faults               #    0.003 M/sec                  
S0-C1           1        26380744077      cycles                    #    1.146 GHz                    
S0-C1           1         9789167019      instructions              #    0.37  insn per cycle         
S0-C1           1         1183992387      branches                  #   51.437 M/sec                  
S0-C1           1          228755152      branch-misses             #   19.32% of all branches        
S0-C2           1           23018.05 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              17803      context-switches          #    0.773 K/sec                  
S0-C2           1               1110      cpu-migrations            #    0.048 K/sec                  
S0-C2           1              63496      page-faults               #    0.003 M/sec                  
S0-C2           1        27421291936      cycles                    #    1.191 GHz                    
S0-C2           1        11647424392      instructions              #    0.42  insn per cycle         
S0-C2           1         1429657529      branches                  #   62.110 M/sec                  
S0-C2           1          255515093      branch-misses             #   17.87% of all branches        
S0-C3           1           23018.05 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              22041      context-switches          #    0.958 K/sec                  
S0-C3           1               1166      cpu-migrations            #    0.051 K/sec                  
S0-C3           1              77145      page-faults               #    0.003 M/sec                  
S0-C3           1        18459942740      cycles                    #    0.802 GHz                    
S0-C3           1         7029920244      instructions              #    0.38  insn per cycle         
S0-C3           1          870759568      branches                  #   37.829 M/sec                  
S0-C3           1          159858243      branch-misses             #   18.36% of all branches        

      23.019724426 seconds time elapsed

================================================================================
================================================================================
Printing results
File: run_forest_importances_faces.txt
instructions: 31028843235
=(7224127786+8250650341+7848688402+7705376706)
frequencies: 1.162
=(1.117+1.211+1.153+1.165)/4
ipcs:0.35
=1*(0.34+0.36+0.36+0.35)/4

File: run_plot_svm_nonlinear.txt
instructions: 36694483292
=(8893808229+9295087950+9251934646+9253652467)
frequencies: 1.276
=(1.230+1.293+1.283+1.300)/4
ipcs:0.57
=1*(0.58+0.57+0.57+0.57)/4

File: run_multioutput_face_completion.txt
instructions: 73079232206
=(15752746667+26853403751+12446848201+18026233587)
frequencies: 0.964
=(0.841+1.279+0.758+0.976)/4
ipcs:0.43
=1*(0.43+0.49+0.38+0.43)/4

File: run_plot_theilsen.txt
instructions: 34800995164
=(6334483509+9789167019+11647424392+7029920244)
frequencies: 0.970
=(0.741+1.146+1.191+0.802)/4
ipcs:0.39
=1*(0.37+0.37+0.42+0.38)/4

================================================================================
Finished running benchmarks
================================================================================
